{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e96debf-ccfa-4741-bd74-68adaf9ab11a",
   "metadata": {},
   "source": [
    "## Fiber Photometry PSTH Analysis\n",
    "\n",
    "This notebook:\n",
    "\n",
    "This notebook aligns ΔF/F photometry traces to behavioral or stimulation events,\n",
    "computes peri-event averages, applies baseline normalization and z-scoring,\n",
    "and generates trial-averaged summaries.\n",
    "\n",
    "Input files should be placed in:\n",
    "data/raw/photometry/\n",
    "\n",
    "# Tested with Python 3.10\n",
    "# Required packages: numpy, pandas, matplotlib, scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854e239c-7e9b-4695-991b-2f3a93c9be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3463ed49-1b81-4ab7-8fb6-8e51abd55e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "DATA_DIR = Path(\"data/raw/photometry\")\n",
    "dff_files = [DATA_DIR / f\"dff_{i}.csv\" for i in range(1,5)] #Range according to the amount of animals/files \n",
    "event_files = [DATA_DIR / f\"{i}_events.csv\" for i in range(1,5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4192f5c-a52c-4c23-bb95-d70d635f2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: build ΔF/F from Doric “signal.csv” with robust 405→470 fit + QC ---\n",
    "\n",
    "def _safe_linfit(x, y, max_iter=3, z=3.0):\n",
    "    \"\"\"\n",
    "    Robust-ish linear fit y ~ a + b*x with iterative outlier trimming (|resid| > z*SD).\n",
    "    Returns slope, intercept.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x = x[m]; y = y[m]\n",
    "    A = np.vstack([x, np.ones_like(x)]).T\n",
    "    for _ in range(int(max_iter)):\n",
    "        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "        yhat = intercept + slope * x\n",
    "        resid = y - yhat\n",
    "        s = np.nanstd(resid)\n",
    "        if not np.isfinite(s) or s == 0:\n",
    "            break\n",
    "        keep = np.abs(resid) <= float(z) * s\n",
    "        if keep.sum() < 10:\n",
    "            break\n",
    "        A = A[keep]; x = x[keep]; y = y[keep]\n",
    "    slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    return float(slope), float(intercept)\n",
    "\n",
    "def preprocess_signal(file_path, max_iter=3, z=3.0, eps_frac=0.01):\n",
    "    \"\"\"\n",
    "    Reads a Doric-exported CSV with columns [time, 470, 405] (already demodulated),\n",
    "    performs a robust linear fit of 405→470, and computes ΔF/F = (470 - fit(405)) / fit(405).\n",
    "\n",
    "    Returns: time(s), dFF (np.array), meta (dict of QC/params).\n",
    "    \"\"\"\n",
    "    # Force Pandas to parse all at once to avoid DtypeWarning\n",
    "    df = pd.read_csv(file_path, header=None, low_memory=False)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"{file_path} needs ≥3 columns: time, Ca(470), iso(405).\")\n",
    "    df = df.iloc[:, :3]\n",
    "    df.columns = [\"time\",\"ca\",\"iso\"]\n",
    "\n",
    "    # Coerce to numeric; drop bad time rows only; keep alignment and interpolate signals\n",
    "    df[\"time\"] = pd.to_numeric(df[\"time\"], errors=\"coerce\")\n",
    "    df[\"ca\"]   = pd.to_numeric(df[\"ca\"],   errors=\"coerce\")\n",
    "    df[\"iso\"]  = pd.to_numeric(df[\"iso\"],  errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"time\"]).reset_index(drop=True)\n",
    "    df[[\"ca\",\"iso\"]] = df[[\"ca\",\"iso\"]].interpolate(limit_direction=\"both\")\n",
    "\n",
    "    if not np.all(np.diff(df[\"time\"].values) > 0):\n",
    "        raise ValueError(\"Timestamps must be strictly increasing.\")\n",
    "    if df[\"iso\"].std() < 1e-12:\n",
    "        raise ValueError(\"Isosbestic has ~zero variance.\")\n",
    "\n",
    "    # Robust fit iso→Ca\n",
    "    slope, intercept = _safe_linfit(df[\"iso\"].values, df[\"ca\"].values, max_iter=max_iter, z=z)\n",
    "    fitted = intercept + slope * df[\"iso\"].values\n",
    "\n",
    "    # Safe denominator (prevents blow-ups if fitted is tiny)\n",
    "    eps = float(eps_frac) * float(np.nanmedian(fitted))\n",
    "    denom = np.maximum(fitted, eps)\n",
    "    dff = (df[\"ca\"].values - fitted) / denom\n",
    "\n",
    "    # QC\n",
    "    r = float(np.corrcoef(df[\"iso\"].values, df[\"ca\"].values)[0,1])\n",
    "    ss_res = float(np.sum((df[\"ca\"].values - fitted)**2))\n",
    "    ss_tot = float(np.sum((df[\"ca\"].values - np.mean(df[\"ca\"].values))**2))\n",
    "    r2 = float(1 - ss_res/ss_tot) if ss_tot > 0 else np.nan\n",
    "    dt = np.median(np.diff(df[\"time\"].values)); Fs = float(1.0/dt) if dt > 0 else np.nan\n",
    "\n",
    "    meta = {\n",
    "        \"slope\": slope, \"intercept\": intercept,\n",
    "        \"r\": r, \"r2\": r2, \"Fs\": Fs, \"n\": int(len(df)),\n",
    "        \"eps\": float(eps), \"eps_frac\": float(eps_frac),\n",
    "        \"robust_max_iter\": int(max_iter), \"robust_z_thresh\": float(z)\n",
    "    }\n",
    "    return df[\"time\"].values, dff, meta\n",
    "\n",
    "def process_files(signal_files, max_iter=3, z=3.0, eps_frac=0.01):\n",
    "    \"\"\"\n",
    "    Runs preprocess_signal() over a list of Doric 'signal.csv' files, plots ΔF/F,\n",
    "    and saves dff_{i}.csv plus dff_{i}_qc.json in the same folder.\n",
    "    \"\"\"\n",
    "    for i, signal_file_path in enumerate(signal_files, start=1):\n",
    "        p = Path(signal_file_path)\n",
    "        print(f\"Processing: {p}\")\n",
    "        try:\n",
    "            t, dff, meta = preprocess_signal(signal_file_path, max_iter=max_iter, z=z, eps_frac=eps_frac)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR on {p.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Warn if the linear relation is weak (helps catch bad coupling/motion)\n",
    "        if np.isfinite(meta.get(\"r2\", np.nan)) and meta[\"r2\"] < 0.30:\n",
    "            print(f\"WARNING ({p.name}): low iso→Ca fit (R²={meta['r2']:.2f}). Check motion/bleach or coupling.\")\n",
    "\n",
    "        # Plot (light smoothing ONLY for visualization)\n",
    "        lo, hi = np.nanpercentile(dff, [1, 99])\n",
    "        pad = 0.05 * (hi - lo if np.isfinite(hi - lo) and hi > lo else 1.0)\n",
    "        ylo, yhi = lo - pad, hi + pad\n",
    "        Fs = meta[\"Fs\"] if np.isfinite(meta[\"Fs\"]) else 30.0\n",
    "        win = max(3, int(round(Fs * 0.5)))  # ~0.5 s rolling median\n",
    "        dff_plot = pd.Series(dff).rolling(win, center=True, min_periods=1).median().values\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(t, dff_plot, label=\"ΔF/F (plot-smooth)\")\n",
    "        plt.xlabel(\"Time (s)\"); plt.ylabel(\"ΔF/F\"); plt.title(f\"ΔF/F — {p.name}\")\n",
    "        plt.ylim(ylo, yhi); plt.legend(); plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save outputs\n",
    "        out_dir = os.path.dirname(signal_file_path)\n",
    "        out_csv = os.path.join(out_dir, f\"dff_{i}.csv\")\n",
    "        pd.DataFrame({\"Timestamp\": t, \"dFF\": dff}).to_csv(out_csv, index=False)\n",
    "\n",
    "        out_qc = os.path.join(out_dir, f\"dff_{i}_qc.json\")\n",
    "        with open(out_qc, \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        print(f\"Saved: dff_{i}.csv, dff_{i}_qc.json | R²={meta['r2']:.3f}, Fs={meta['Fs']:.2f} Hz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97ff88-e06d-4682-9e09-ce04a86d2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ΔF/F traces and event onsets; build common peri-stim grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462694c1-bd7d-44c6-a184-2878682bc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- I/O -----\n",
    "def load_preprocessed_dff(dff_file_path):\n",
    "    df = pd.read_csv(dff_file_path)\n",
    "    t = pd.to_numeric(df['Timestamp'], errors='coerce').values\n",
    "    y = pd.to_numeric(df['dFF'], errors='coerce').values\n",
    "    m = np.isfinite(t) & np.isfinite(y)\n",
    "    t, y = t[m], y[m]\n",
    "    order = np.argsort(t)\n",
    "    return t[order], y[order]\n",
    "\n",
    "def load_events(file_path):\n",
    "    ev = pd.read_csv(file_path, header=None)\n",
    "    et = pd.to_numeric(ev.iloc[:, 1], errors='coerce').dropna().values\n",
    "    return np.sort(et)\n",
    "\n",
    "# ----- time grid + interpolation -----\n",
    "def build_common_grid(timestamps, pre_time, post_time):\n",
    "    diffs = np.diff(timestamps)\n",
    "    dt = np.nanmedian(diffs[np.isfinite(diffs)])\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        raise ValueError(\"Could not infer a valid positive sampling interval from timestamps.\")\n",
    "    # include the right edge\n",
    "    grid = np.arange(pre_time, post_time + 0.5*dt, dt)\n",
    "    return grid, dt\n",
    "\n",
    "def interpolate_trial_to_grid(t_abs, y_abs, event_time, rel_grid, pre_time, post_time):\n",
    "    t0 = event_time + pre_time\n",
    "    t1 = event_time + post_time\n",
    "    m = (t_abs >= t0) & (t_abs <= t1)\n",
    "    if not np.any(m):\n",
    "        return None\n",
    "    t_win = t_abs[m] - event_time     # relative times\n",
    "    y_win = y_abs[m]\n",
    "    # need at least two points with some span to interpolate\n",
    "    if len(t_win) < 2 or (np.nanmax(t_win) - np.nanmin(t_win)) < 1e-9:\n",
    "        return None\n",
    "    y_interp = np.interp(rel_grid, t_win, y_win, left=np.nan, right=np.nan)\n",
    "    return y_interp\n",
    "\n",
    "# ----- robust stats -----\n",
    "def mad(x):\n",
    "    med = np.nanmedian(x)\n",
    "    return np.nanmedian(np.abs(x - med))\n",
    "\n",
    "def sem(a, axis=0):\n",
    "    # Standard Error of the Mean with unbiased std (ddof=1)\n",
    "    n = np.sum(np.isfinite(a), axis=axis)\n",
    "    s = np.nanstd(a, axis=axis, ddof=1)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        return s / np.sqrt(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4799a4c0-2896-4ba4-a98f-ec152ee5dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# compute + export\n",
    "# =========================\n",
    "\n",
    "def compute_measures(t, y, event_times,\n",
    "                     pre_time=-3.0, post_time=6.0, baseline_end=-0.1,\n",
    "                     min_baseline_pts=20, robust_scale_factor=1.4826,\n",
    "                     min_baseline_sec=1.0, min_valid_frac=0.80):\n",
    "    \"\"\"\n",
    "    Compute per recording:\n",
    "      - baseline-subtracted ΔF/F PSTH (baseline = median over [pre_time, baseline_end])\n",
    "      - Standard z-score (baseline mean/SD, ddof=1)\n",
    "      - Robust z-score (baseline median/MAD × 1.4826; falls back to SD if MAD==0)\n",
    "\n",
    "    Returns:\n",
    "      rel_grid,\n",
    "      dff_mean, dff_sem,\n",
    "      z_std_mean, z_std_sem,\n",
    "      z_rob_mean, z_rob_sem,\n",
    "      n_trials_kept_dff, n_trials_kept_zstd, n_trials_kept_zrob\n",
    "    \"\"\"\n",
    "    rel_grid, dt = build_common_grid(t, pre_time, post_time)\n",
    "\n",
    "    trials_dff   = []  # ΔF/F baseline-subtracted by baseline median\n",
    "    trials_z_std = []  # (x - mean) / std\n",
    "    trials_z_rob = []  # (x - median) / (1.4826 * MAD)\n",
    "\n",
    "    for et in event_times:\n",
    "        yi = interpolate_trial_to_grid(t, y, et, rel_grid, pre_time, post_time)\n",
    "        if yi is None:\n",
    "            continue\n",
    "\n",
    "        # Baseline on the RELATIVE grid\n",
    "        bmask = (rel_grid >= pre_time) & (rel_grid <= baseline_end)\n",
    "        b = yi[bmask]\n",
    "\n",
    "        # --- Time-based + completeness guards for baseline ---\n",
    "        required_pts = max(int(np.ceil(min_baseline_sec / dt)), int(min_baseline_pts))\n",
    "        valid = np.isfinite(b)\n",
    "        if valid.sum() < required_pts or (valid.sum() / max(b.size, 1)) < min_valid_frac:\n",
    "            # Skip trials with too little valid baseline\n",
    "            continue\n",
    "\n",
    "        b_med  = np.nanmedian(b)\n",
    "        b_mean = np.nanmean(b)\n",
    "        b_std  = np.nanstd(b, ddof=1)\n",
    "\n",
    "        # ΔF/F baseline-subtracted\n",
    "        trials_dff.append(yi - b_med)\n",
    "\n",
    "        # Standard z (mean/SD)\n",
    "        if np.isfinite(b_std) and b_std > 0:\n",
    "            trials_z_std.append((yi - b_mean) / b_std)\n",
    "\n",
    "        # Robust z (median/MAD×1.4826). If MAD==0, fall back to SD.\n",
    "        b_mad = mad(b)\n",
    "        if np.isfinite(b_mad) and b_mad > 0:\n",
    "            trials_z_rob.append((yi - b_med) / (robust_scale_factor * b_mad))\n",
    "        else:\n",
    "            if np.isfinite(b_std) and b_std > 0:\n",
    "                trials_z_rob.append((yi - b_med) / b_std)\n",
    "\n",
    "    # Aggregate -> mean ± SEM across trials\n",
    "    out = {}\n",
    "\n",
    "    if len(trials_dff) > 0:\n",
    "        dff_arr = np.vstack(trials_dff)\n",
    "        out['dff_mean'] = np.nanmean(dff_arr, axis=0)\n",
    "        out['dff_sem']  = sem(dff_arr, axis=0)\n",
    "        out['n_dff']    = dff_arr.shape[0]\n",
    "    else:\n",
    "        out['dff_mean'] = out['dff_sem'] = None\n",
    "        out['n_dff']    = 0\n",
    "\n",
    "    if len(trials_z_std) > 0:\n",
    "        zstd_arr = np.vstack(trials_z_std)\n",
    "        out['z_std_mean'] = np.nanmean(zstd_arr, axis=0)\n",
    "        out['z_std_sem']  = sem(zstd_arr, axis=0)\n",
    "        out['n_zstd']     = zstd_arr.shape[0]\n",
    "    else:\n",
    "        out['z_std_mean'] = out['z_std_sem'] = None\n",
    "        out['n_zstd']     = 0\n",
    "\n",
    "    if len(trials_z_rob) > 0:\n",
    "        zrob_arr = np.vstack(trials_z_rob)\n",
    "        out['z_rob_mean'] = np.nanmean(zrob_arr, axis=0)\n",
    "        out['z_rob_sem']  = sem(zrob_arr, axis=0)\n",
    "        out['n_zrob']     = zrob_arr.shape[0]\n",
    "    else:\n",
    "        out['z_rob_mean'] = out['z_rob_sem'] = None\n",
    "        out['n_zrob']     = 0\n",
    "\n",
    "    return (rel_grid,\n",
    "            out['dff_mean'], out['dff_sem'],\n",
    "            out['z_std_mean'], out['z_std_sem'],\n",
    "            out['z_rob_mean'], out['z_rob_sem'],\n",
    "            out['n_dff'], out['n_zstd'], out['n_zrob'])\n",
    "\n",
    "\n",
    "def process_files_psth_both(dff_files, event_files,\n",
    "                            pre_time=-3.0, post_time=6.0, baseline_end=-0.1,\n",
    "                            min_baseline_pts=20, save_figs=False,\n",
    "                            min_baseline_sec=1.0, min_valid_frac=0.80):\n",
    "    \"\"\"\n",
    "    For each recording:\n",
    "      - Plots ΔF/F baseline-subtracted, standard Z, robust Z\n",
    "      - Saves per-recording CSV with aligned time and mean ± SEM for all three\n",
    "      - Saves AUC-by-1s-bin CSV for all three\n",
    "    After all recordings:\n",
    "      - Saves ONE aggregate CSV with the same time column and one column per recording for each metric\n",
    "        Column order:\n",
    "          time_s |\n",
    "          dFF_mean_rec1..N | dFF_sem_rec1..N |\n",
    "          z_std_mean_rec1..N | z_std_sem_rec1..N |\n",
    "          z_robust_mean_rec1..N | z_robust_sem_rec1..N\n",
    "    \"\"\"\n",
    "    per_rec = {}          # i -> dict with rel_t and all measures\n",
    "    first_out_dir = None  # use this folder for aggregate CSV\n",
    "\n",
    "    for i, (dff_file_path, event_file_path) in enumerate(zip(dff_files, event_files), start=1):\n",
    "        if first_out_dir is None:\n",
    "            first_out_dir = os.path.dirname(dff_file_path)\n",
    "\n",
    "        print(f\"\\n=== Recording {i} ===\")\n",
    "        print(f\"ΔF/F file: {dff_file_path}\")\n",
    "        print(f\"Events  : {event_file_path}\")\n",
    "\n",
    "        t, y = load_preprocessed_dff(dff_file_path)\n",
    "        et = load_events(event_file_path)\n",
    "\n",
    "        (rel_t,\n",
    "         dff_mean, dff_sem,\n",
    "         zstd_mean, zstd_sem,\n",
    "         zrob_mean, zrob_sem,\n",
    "         n_dff, n_zstd, n_zrob) = compute_measures(\n",
    "            t, y, et,\n",
    "            pre_time=pre_time, post_time=post_time, baseline_end=baseline_end,\n",
    "            min_baseline_pts=min_baseline_pts,\n",
    "            min_baseline_sec=min_baseline_sec, min_valid_frac=min_valid_frac\n",
    "        )\n",
    "\n",
    "        # ---- store for later aggregation ----\n",
    "        per_rec[i] = {\n",
    "            'rel_t': rel_t,\n",
    "            'dFF_mean': dff_mean, 'dFF_sem': dff_sem,\n",
    "            'z_std_mean': zstd_mean, 'z_std_sem': zstd_sem,\n",
    "            'z_robust_mean': zrob_mean, 'z_robust_sem': zrob_sem\n",
    "        }\n",
    "\n",
    "        # ---- Plot ΔF/F baseline-subtracted ----\n",
    "        if dff_mean is not None:\n",
    "            plt.figure()\n",
    "            plt.plot(rel_t, dff_mean, label=f'Rec {i} ΔF/F (baseline-sub)')\n",
    "            plt.fill_between(rel_t, dff_mean - dff_sem, dff_mean + dff_sem, alpha=0.3)\n",
    "            plt.axvline(0, color='r', linestyle='--')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('ΔF/F (baseline-subtracted)')\n",
    "            plt.title(f'PSTH ΔF/F — Rec {i} (n={n_dff} trials)')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            if save_figs:\n",
    "                figpath = os.path.join(os.path.dirname(dff_file_path), f'psth_dff_rec{i}.png')\n",
    "                plt.savefig(figpath, dpi=200)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No valid trials for ΔF/F baseline-subtracted.\")\n",
    "\n",
    "        # ---- Plot Standard Z (mean/SD) ----\n",
    "        if zstd_mean is not None:\n",
    "            plt.figure()\n",
    "            plt.plot(rel_t, zstd_mean, label=f'Rec {i} Z (mean/SD)')\n",
    "            plt.fill_between(rel_t, zstd_mean - zstd_sem, zstd_mean + zstd_sem, alpha=0.25)\n",
    "            plt.axvline(0, color='r', linestyle='--')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Z-score (baseline mean/SD)')\n",
    "            plt.title(f'Standard Z-score — Rec {i} (n={n_zstd} trials)')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            if save_figs:\n",
    "                figpath = os.path.join(os.path.dirname(dff_file_path), f'z_std_rec{i}.png')\n",
    "                plt.savefig(figpath, dpi=200)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No valid trials for Standard Z (mean/SD).\")\n",
    "\n",
    "        # ---- Plot Robust Z (median/MAD×1.4826) ----\n",
    "        if zrob_mean is not None:\n",
    "            plt.figure()\n",
    "            plt.plot(rel_t, zrob_mean, label=f'Rec {i} Robust Z (median/MAD×1.4826)')\n",
    "            plt.fill_between(rel_t, zrob_mean - zrob_sem, zrob_mean + zrob_sem, alpha=0.25)\n",
    "            plt.axvline(0, color='r', linestyle='--')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Robust Z-score (median/MAD×1.4826)')\n",
    "            plt.title(f'Robust Z-score — Rec {i} (n={n_zrob} trials)')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            if save_figs:\n",
    "                figpath = os.path.join(os.path.dirname(dff_file_path), f'z_robust_rec{i}.png')\n",
    "                plt.savefig(figpath, dpi=200)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No valid trials for Robust Z (median/MAD).\")\n",
    "\n",
    "        # ---- Export per-recording aligned CSV ----\n",
    "        out_df = pd.DataFrame({'time_s': rel_t})\n",
    "        # always include columns; fill with NaN if None\n",
    "        out_df['dFF_mean'] = dff_mean if dff_mean is not None else np.nan\n",
    "        out_df['dFF_sem']  = dff_sem  if dff_sem  is not None else np.nan\n",
    "        out_df['z_std_mean'] = zstd_mean if zstd_mean is not None else np.nan\n",
    "        out_df['z_std_sem']  = zstd_sem  if zstd_sem  is not None else np.nan\n",
    "        out_df['z_robust_mean'] = zrob_mean if zrob_mean is not None else np.nan\n",
    "        out_df['z_robust_sem']  = zrob_sem  if zrob_sem  is not None else np.nan\n",
    "\n",
    "        export_path = os.path.join(os.path.dirname(dff_file_path), f'psth_and_z_results_rec{i}.csv')\n",
    "        out_df.to_csv(export_path, index=False)\n",
    "        print(f\"Saved: {export_path}\")\n",
    "\n",
    "        # ---- AUC per 1 s bin for all three (optional but handy) ----\n",
    "        auc_rows = []\n",
    "        for start in range(int(pre_time), int(post_time)):\n",
    "            end = start + 1\n",
    "            m = (rel_t >= start) & (rel_t < end)\n",
    "\n",
    "            def auc_or_nan(arr):\n",
    "                return np.trapz(arr[m], x=rel_t[m]) if arr is not None and np.any(m) else np.nan\n",
    "\n",
    "            auc_rows.append({\n",
    "                'TimeBin': f'{start} to {end} s',\n",
    "                'AUC_dFF': auc_or_nan(dff_mean),\n",
    "                'AUC_z_std': auc_or_nan(zstd_mean),\n",
    "                'AUC_z_robust': auc_or_nan(zrob_mean)\n",
    "            })\n",
    "\n",
    "        auc_df = pd.DataFrame(auc_rows)\n",
    "        auc_export_path = os.path.join(os.path.dirname(dff_file_path), f'auc_by_bin_rec{i}.csv')\n",
    "        auc_df.to_csv(auc_export_path, index=False)\n",
    "        print(f\"Saved: {auc_export_path}\")\n",
    "\n",
    "    # =========================\n",
    "    # Aggregate CSV (all recs)\n",
    "    # =========================\n",
    "    if len(per_rec) == 0:\n",
    "        print(\"No recordings to aggregate.\")\n",
    "        return\n",
    "\n",
    "    # Use the first recording's time grid as the common base\n",
    "    base_idx = sorted(per_rec.keys())[0]\n",
    "    base_t = per_rec[base_idx]['rel_t']\n",
    "\n",
    "    def to_base(arr, t_src, t_dst):\n",
    "        \"\"\"Interpolate array onto destination time base; out-of-range -> NaN.\"\"\"\n",
    "        if arr is None or t_src is None or len(t_src) == 0:\n",
    "            return np.full_like(t_dst, np.nan, dtype=float)\n",
    "        out = np.interp(t_dst, t_src, arr)\n",
    "        # NaN out-of-range\n",
    "        out[t_dst < np.nanmin(t_src)] = np.nan\n",
    "        out[t_dst > np.nanmax(t_src)] = np.nan\n",
    "        return out\n",
    "\n",
    "    agg_df = pd.DataFrame({'time_s': base_t})\n",
    "\n",
    "    metric_order = [\n",
    "        ('dFF_mean',        'dFF_mean_rec{idx}'),\n",
    "        ('dFF_sem',         'dFF_sem_rec{idx}'),\n",
    "        ('z_std_mean',      'z_std_mean_rec{idx}'),\n",
    "        ('z_std_sem',       'z_std_sem_rec{idx}'),\n",
    "        ('z_robust_mean',   'z_robust_mean_rec{idx}'),\n",
    "        ('z_robust_sem',    'z_robust_sem_rec{idx}'),\n",
    "    ]\n",
    "\n",
    "    n_recs = len(per_rec)\n",
    "    for metric_key, col_template in metric_order:\n",
    "        for idx in range(1, n_recs + 1):\n",
    "            rel_t = per_rec[idx]['rel_t']\n",
    "            arr   = per_rec[idx][metric_key]\n",
    "            agg_df[col_template.format(idx=idx)] = to_base(arr, rel_t, base_t)\n",
    "\n",
    "    agg_path = os.path.join(first_out_dir, 'psth_z_all_recordings_aligned.csv')\n",
    "    agg_df.to_csv(agg_path, index=False)\n",
    "    print(f\"Saved aggregate CSV with all recordings: {agg_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785728f7-a436-44bf-bff4-9f8cf9456ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- RUN ANALYSIS (requires files in DATA_DIR) ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d4963-873b-49ba-a2bb-0f0ae535441b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommended baseline guards: ≥1.0 s valid baseline and ≥80% finite samples in that baseline window\n",
    "process_files_psth_both(\n",
    "    dff_files,\n",
    "    event_files,\n",
    "    pre_time=-3.0,\n",
    "    post_time=6.0,\n",
    "    baseline_end=-0.1,\n",
    "    min_baseline_pts=20,\n",
    "    min_baseline_sec=1.0,\n",
    "    min_valid_frac=0.80,\n",
    "    save_figs=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bc3f1-16cf-48e2-9c67-5ad13e3bbe7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
